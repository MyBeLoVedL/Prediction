{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Sentiment Analysis using News Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Data.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Date     0\nLabel    0\nTop1     0\nTop2     0\nTop3     0\nTop4     0\nTop5     0\nTop6     0\nTop7     0\nTop8     0\nTop9     0\nTop10    0\nTop11    0\nTop12    0\nTop13    0\nTop14    0\nTop15    0\nTop16    0\nTop17    0\nTop18    0\nTop19    0\nTop20    0\nTop21    0\nTop22    0\nTop23    1\nTop24    3\nTop25    3\ndtype: int64"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set({})\n",
    "with open('stopwords','r') as f:\n",
    "\tfor line in f:\n",
    "\t\tstop_words.add(line.strip())\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-75c4564ad5a4>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Top23'][index] = df['Top1'][index]\n",
      "<ipython-input-36-75c4564ad5a4>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Top24'][index] = df['Top1'][index]\n",
      "<ipython-input-36-75c4564ad5a4>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Top25'][index] = df['Top1'][index]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Date     0\nLabel    0\nTop1     0\nTop2     0\nTop3     0\nTop4     0\nTop5     0\nTop6     0\nTop7     0\nTop8     0\nTop9     0\nTop10    0\nTop11    0\nTop12    0\nTop13    0\nTop14    0\nTop15    0\nTop16    0\nTop17    0\nTop18    0\nTop19    0\nTop20    0\nTop21    0\nTop22    0\nTop23    0\nTop24    0\nTop25    0\ndtype: int64"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = df['Top23'].index\n",
    "df['Top23'][index] = df['Top1'][index]\n",
    "\n",
    "index = df['Top24'].index\n",
    "df['Top24'][index] = df['Top1'][index]\n",
    "\n",
    "\n",
    "index = df['Top25'].index\n",
    "df['Top25'][index] = df['Top1'][index]\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'a hindrance operations extracts leaked reports scorecard hughes instant hit buoys blues jack gets skates icecold alex chaos maracana builds united depleted leicester prevail elliott spoils evertons party hungry spurs sense rich pickings gunners wide easy target derby raise glass strupars debut double southgate strikes leeds pay penalty hammers hand robson youthful lesson saints party like wear wolves turned lambs stump mike catches testy goughs taunt langer escapes hit flintoff injury piles woe england hunters threaten jospin new battle somme kohls successor drawn scandal the difference men women sara denver nurse turned solicitor dianas landmine crusade put tories panic yeltsins resignation caught opposition flatfooted a hindrance operations extracts leaked reports a hindrance operations extracts leaked reports a hindrance operations extracts leaked reports'"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re_obj = re.compile(r'[^a-zA-Z ]')\n",
    "row_string = []\n",
    "\n",
    "\n",
    "for i in range(len(df.index)):\n",
    "\trow_string.append(' '.join(top for top in df.iloc[i,2:]))\n",
    "for i,_ in enumerate(row_string):\n",
    "\t# row_string[i].replace(r'[^a-zA-Z]','',regex = True,inplace=True)\n",
    "\trow_string[i] =  re_obj.sub('',row_string[i])\n",
    "\trow_string[i] = row_string[i].lower()\n",
    "row_string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'A hindrance operations extracts leaked reports Scorecard Hughes instant hit buoys Blues Jack gets skates icecold Alex Chaos Maracana builds United Depleted Leicester prevail Elliott spoils Evertons party Hungry Spurs sense rich pickings Gunners wide easy target Derby raise glass Strupars debut double Southgate strikes Leeds pay penalty Hammers hand Robson youthful lesson Saints party like Wear wolves turned lambs Stump mike catches testy Goughs taunt Langer escapes hit Flintoff injury piles woe England Hunters threaten Jospin new battle Somme Kohls successor drawn scandal The difference men women Sara Denver nurse turned solicitor Dianas landmine crusade put Tories panic Yeltsins resignation caught opposition flatfooted A hindrance operations extracts leaked reports A hindrance operations extracts leaked reports A hindrance operations extracts leaked reports'"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stop(sentence : str):\n",
    "\twords = sentence.split()\n",
    "\treturn ' '.join([word  for word in words if word not in stop_words])\n",
    "\n",
    "\n",
    "for i,s in enumerate(row_string):\n",
    "\trow_string[i] = remove_stop(row_string[i])\n",
    "\n",
    "row_string[0]\n",
    "\n",
    "# \tdf.iloc[:,2 + i] = df.iloc[:,2 + i].map(remove_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Label</th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2000-01-03</td>\n      <td>0</td>\n      <td>a hindrance operations extracts leaked reports...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2000-01-04</td>\n      <td>0</td>\n      <td>scorecard the best lake scene leader german sl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2000-01-05</td>\n      <td>0</td>\n      <td>coventry caught counter flo uniteds rivals roa...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2000-01-06</td>\n      <td>1</td>\n      <td>pilgrim knows progress thatcher facing ban mci...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2000-01-07</td>\n      <td>1</td>\n      <td>hitches horlocks beckham united survive breast...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "         Date  Label                                            Content\n0  2000-01-03      0  a hindrance operations extracts leaked reports...\n1  2000-01-04      0  scorecard the best lake scene leader german sl...\n2  2000-01-05      0  coventry caught counter flo uniteds rivals roa...\n3  2000-01-06      1  pilgrim knows progress thatcher facing ban mci...\n4  2000-01-07      1  hitches horlocks beckham united survive breast..."
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Top1'] = pd.Series(row_string)\n",
    "df = df[['Date','Label','Top1']]\n",
    "df.columns = ['Date','Label','Content']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['Date'] < '20150101']\n",
    "test = df[df['Date'] > '20141231']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'scorecard the best lake scene leader german sleaze inquiry cheerio boyo the main recommendations has cubie killed fees has cubie killed fees has cubie killed fees hopkins furious fosters lack hannibal appetite has cubie killed fees a tale two tails i say i like i like i say elbows eyes nipples task force assess risk asteroid collision how i found last on critical list the timing lives dear doctor irish court halts ira mans extradition northern ireland burundi peace initiative fades rebels reject mandela mediator pe points way forward ecb campaigners keep pressure nazi war crimes suspect scorecard scorecard scorecard'"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines = list(train['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement BAG OF WORDS\n",
    "countvector=CountVectorizer(ngram_range=(2,2))\n",
    "traindataset=countvector.fit_transform(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(criterion='entropy', n_estimators=200)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement RandomForest Classifier\n",
    "randomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\n",
    "randomclassifier.fit(traindataset,train['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict for the Test Dataset\n",
    "test_transform= []\n",
    "for row in range(0,len(test.index)):\n",
    "    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "test_dataset = countvector.transform(test_transform)\n",
    "predictions = randomclassifier.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import library to check accuracy\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[138  48]\n",
      " [  9 183]]\n",
      "0.8492063492063492\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.74      0.83       186\n",
      "           1       0.79      0.95      0.87       192\n",
      "\n",
      "    accuracy                           0.85       378\n",
      "   macro avg       0.87      0.85      0.85       378\n",
      "weighted avg       0.86      0.85      0.85       378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix=confusion_matrix(test['Label'],predictions)\n",
    "print(matrix)\n",
    "score=accuracy_score(test['Label'],predictions)\n",
    "print(score)\n",
    "report=classification_report(test['Label'],predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\n",
    "\n",
    "basicmodel = LogisticRegression()\n",
    "basicmodel = basicmodel.fit(traindataset, train[\"Label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logic regression [0.8571428571428571]\n"
     ]
    }
   ],
   "source": [
    "preds1 = basicmodel.predict(test_dataset)\n",
    "\n",
    "acc1=accuracy_score(test['Label'], preds1)\n",
    "print(f'logic regression [{acc1}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * naive beyes \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.7, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(headlines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive beyes :  [0.5079365079365079]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "advancedmodel = MultinomialNB(alpha=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds4 = advancedmodel.predict(advancedtest)\n",
    "acc2=accuracy_score(test['Label'], preds4)\n",
    "print(f'naive beyes :  [{acc2}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gradient boosting machine\n",
    "\n",
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.9, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(headlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive beyes :  [0.7116402116402116]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "advancedmodel = GradientBoostingClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds8 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc3 = accuracy_score(test['Label'], preds8)\n",
    "\n",
    "print(f'naive beyes :  [{acc3}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.2, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive beye :  [0.5079365079365079]\n"
     ]
    }
   ],
   "source": [
    "# stochastic gradient descent   \n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\n",
    "\n",
    "advancedmodel = SGDClassifier(loss='modified_huber',random_state=0, shuffle=True)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds10 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc4 = accuracy_score(test['Label'], preds10)\n",
    "\n",
    "\n",
    "print(f'na :  [{acc4}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "maxlen = 200\n",
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=max_features)\n",
    "tokenizer.fit_on_texts(headlines)\n",
    "sequences_train = tokenizer.texts_to_sequences(headlines)\n",
    "sequences_test = tokenizer.texts_to_sequences(testheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c056f39b8fbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "\n",
    "\n",
    "X_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=3,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds5 = model.predict_classes(X_test, verbose=0)\n",
    "acc15 = accuracy_score(test['Label'], preds5)\n",
    "\n",
    "\n",
    "print(f'LSTM : {preds5}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}